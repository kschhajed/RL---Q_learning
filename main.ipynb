{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d61ffe9e",
      "metadata": {
        "id": "d61ffe9e"
      },
      "source": [
        "# <center>CSE 4/574: Introduction to Machine Learning</center>\n",
        "## <center>Prof. Sargur Srihari</center>\n",
        "### <center>Assignment 3</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "1f1cf939",
      "metadata": {
        "id": "1f1cf939"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import gym\n",
        "from gym import spaces\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0db09cc",
      "metadata": {
        "id": "a0db09cc"
      },
      "source": [
        "### Stock Trading Environment "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "0f827591",
      "metadata": {
        "id": "0f827591"
      },
      "outputs": [],
      "source": [
        "# Defining the Stock Trading Environment.\n",
        "\"\"\"DON'T MAKE ANY CHANGES TO THE ENVIRONMENT.\"\"\"\n",
        "\n",
        "\n",
        "class StockTradingEnvironment(gym.Env):\n",
        "    \"\"\"This class implements the Stock Trading environment.\"\"\"\n",
        "\n",
        "    def __init__(self, file_path, train=True, number_of_days_to_consider=10):\n",
        "        \"\"\"This method initializes the environment.\n",
        "\n",
        "        :param file_path: - Path of the CSV file containing the historical stock data.\n",
        "        :param train: - Boolean indicating whether the goal is to train or test the performance of the agent.\n",
        "        :param number_of_days_to_consider = Integer representing whether the number of days the for which the agent\n",
        "                considers the trend in stock price to make a decision.\"\"\"\n",
        "\n",
        "        self.file_path = file_path  # Path of the CSV file containing the historical stock data.\n",
        "        self.stock_data = pd.read_csv(self.file_path)  # Reading the CSV file containing the historical stock data.\n",
        "        self.train = train  # Boolean indicating to use the training stock data by default.\n",
        "        # Splitting the data into train and test datasets.\n",
        "        self.training_stock_data = self.stock_data.iloc[:int(0.8 * len(self.stock_data))]\n",
        "        self.testing_stock_data = self.stock_data.iloc[int(0.8 * len(self.stock_data)):].reset_index()\n",
        "        self.observation_space = spaces.Discrete(4)  # This defines that there are four states in the environment.\n",
        "        # This defines that there are 3 discrete actions that the agent can perform (Buy, Sell, Hold).\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "        self.investment_capital = 100000  # This defines the investment capital that the agent starts with.\n",
        "        self.number_of_shares = 0  # This defines number of shares currently held by the agent.\n",
        "        self.stock_value = 0  # This defines the value of the stock currently held by the agent.\n",
        "        self.book_value = 0  # This defines the total value for which the agent bought the shares.\n",
        "        # This defines the agent's total account value.\n",
        "        self.total_account_value = self.investment_capital + self.stock_value\n",
        "        # List to store the total account value over training or evaluation.\n",
        "        self.total_account_value_list = []\n",
        "        # This defines the number of days for which the agent considers the data before taking an action.\n",
        "        self.number_of_days_to_consider = number_of_days_to_consider\n",
        "        # The maximum timesteps the agent will take before the episode ends.\n",
        "        if self.train:\n",
        "            self.max_timesteps = len(self.training_stock_data) - self.number_of_days_to_consider\n",
        "        else:\n",
        "            self.max_timesteps = len(self.testing_stock_data) - self.number_of_days_to_consider\n",
        "        # Initializing the number of steps taken to 0.\n",
        "        self.timestep = 0\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"This method resets the environment and returns the observation.\n",
        "\n",
        "        :returns observation: - (Integer in the range of 0 to 3 representing the four possible observations that the\n",
        "                                 agent can receive. The observation depends upon whether the price increased on average\n",
        "                                 in the number of days the agent considers, and whether the agent already has the stock\n",
        "                                 or not.)\"\"\"\n",
        "\n",
        "        self.investment_capital = 100000  # This defines the investment capital that the agent starts with.\n",
        "        self.number_of_shares = 0  # This defines number of shares currently held by the agent.\n",
        "        self.stock_value = 0  # This defines the value of the stock currently held by the agent.\n",
        "        self.book_value = 0  # This defines the total value for which the agent bought the shares.\n",
        "        # This defines the agent's total account value.\n",
        "        self.total_account_value = self.investment_capital + self.stock_value\n",
        "        # List to store the total account value over training or evaluation.\n",
        "        self.total_account_value_list = []\n",
        "        # Initializing the number of steps taken to 0.\n",
        "        self.timestep = 0\n",
        "\n",
        "        # Getting the observation vector.\n",
        "        if self.train:\n",
        "            # If the task is to train the agent the maximum timesteps will be equal to the number of days considered\n",
        "            # subtracted from the  length of the training stock data.\n",
        "            self.max_timesteps = len(self.training_stock_data) - self.number_of_days_to_consider\n",
        "\n",
        "            # Calculating whether the price increased or decreased/remained the same on the majority of days the agent\n",
        "            # considers.\n",
        "            price_increase_list = []\n",
        "            for i in range(self.number_of_days_to_consider):\n",
        "                if self.training_stock_data['Close'][self.timestep + 1 + i] \\\n",
        "                        - self.training_stock_data['Close'][self.timestep + i] > 0:\n",
        "                    price_increase_list.append(1)\n",
        "                else:\n",
        "                    price_increase_list.append(0)\n",
        "\n",
        "            if (np.sum(price_increase_list) / self.number_of_days_to_consider) >= 0.5:\n",
        "                price_increase = 1\n",
        "                price_decrease = 0\n",
        "            else:\n",
        "                price_increase = 0\n",
        "                price_decrease = 1\n",
        "\n",
        "            # Observation vector that will be passed to the agent.\n",
        "            observation = [price_increase, price_decrease, 0, 1]\n",
        "\n",
        "        else:\n",
        "            # If the task is to evaluate the trained agent's performance the maximum timesteps will be equal to the\n",
        "            # number of days considered subtracted from the  length of the testing stock data.\n",
        "            self.max_timesteps = len(self.testing_stock_data) - self.number_of_days_to_consider\n",
        "\n",
        "            # Calculating whether the price increased or decreased/remained the same on the majority of days the agent\n",
        "            # considers.\n",
        "            price_increase_list = []\n",
        "            for i in range(self.number_of_days_to_consider):\n",
        "                if self.testing_stock_data['Close'][self.timestep + 1 + i] \\\n",
        "                        - self.testing_stock_data['Close'][self.timestep + i] > 0:\n",
        "                    price_increase_list.append(1)\n",
        "                else:\n",
        "                    price_increase_list.append(0)\n",
        "\n",
        "            if (np.sum(price_increase_list) / self.number_of_days_to_consider) >= 0.5:\n",
        "                price_increase = 1\n",
        "                price_decrease = 0\n",
        "            else:\n",
        "                price_increase = 0\n",
        "                price_decrease = 1\n",
        "\n",
        "            # Observation vector.\n",
        "            observation = [price_increase, price_decrease, 0, 1]\n",
        "        if np.array_equal(observation, [1, 0, 0, 1]):\n",
        "            observation = 0\n",
        "        if np.array_equal(observation, [1, 0, 1, 0]):\n",
        "            observation = 1\n",
        "        if np.array_equal(observation, [0, 1, 0, 1]):\n",
        "            observation = 2\n",
        "        if np.array_equal(observation, [0, 1, 1, 0]):\n",
        "            observation = 3\n",
        "\n",
        "        return observation\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"This method implements what happens when the agent takes the action to Buy/Sell/Hold.\n",
        "\n",
        "        :param action: - (Integer in the range 0 to 2 inclusive.)\n",
        "\n",
        "        :returns observation: - (Integer in the range of 0 to 3 representing the four possible observations that the\n",
        "                                 agent can receive. The observation depends upon whether the price increased on average\n",
        "                                 in the number of days the agent considers, and whether the agent already has the stock\n",
        "                                 or not.)\n",
        "                 reward: - (Integer/Float value that's used to measure the performance of the agent.)\n",
        "                 done: - (Boolean describing whether or not the episode has ended.)\n",
        "                 info: - (A dictionary that can be used to provide additional implementation information.)\"\"\"\n",
        "\n",
        "        # We give the agent a penalty for taking actions such as buying a stock when the agent doesn't have the\n",
        "        # investment capital and selling a stock when the agent doesn't have any shares.\n",
        "        penalty = 0\n",
        "\n",
        "        if self.train:\n",
        "            if action == 0:  # Buy\n",
        "                if self.number_of_shares > 0:\n",
        "                    penalty = -10\n",
        "                # Determining the number of shares the agent can buy.\n",
        "                number_of_shares_to_buy = math.floor(self.investment_capital / self.training_stock_data[\n",
        "                    'Open'][self.timestep + self.number_of_days_to_consider])\n",
        "                # Adding to the number of shares the agent has.\n",
        "                self.number_of_shares += number_of_shares_to_buy\n",
        "\n",
        "                # Computing the stock value, book value, investment capital and reward.\n",
        "                if number_of_shares_to_buy > 0:\n",
        "                    self.stock_value +=\\\n",
        "                        self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
        "                        * number_of_shares_to_buy\n",
        "                    self.book_value += \\\n",
        "                        self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider]\\\n",
        "                        * number_of_shares_to_buy\n",
        "                    self.investment_capital -= \\\n",
        "                        self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
        "                        * number_of_shares_to_buy\n",
        "\n",
        "                    reward = 1 + penalty\n",
        "\n",
        "                else:\n",
        "                    # Computing the stock value and reward.\n",
        "                    self.stock_value = \\\n",
        "                        self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
        "                        * self.number_of_shares\n",
        "                    reward = -10\n",
        "\n",
        "            if action == 1:  # Sell\n",
        "                # Computing the investment capital, sell value and reward.\n",
        "                self.investment_capital += \\\n",
        "                    self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
        "                    * self.number_of_shares\n",
        "                sell_value = self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
        "                             * self.number_of_shares\n",
        "\n",
        "                if self.book_value > 0:\n",
        "                    reward = (sell_value - self.book_value) / self.book_value * 100\n",
        "                else:\n",
        "                    reward = -10\n",
        "\n",
        "                self.number_of_shares = 0\n",
        "                self.stock_value = 0\n",
        "                self.book_value = 0\n",
        "\n",
        "            if action == 2:  # Hold\n",
        "                # Computing the stock value and reward.\n",
        "                self.stock_value = self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
        "                                   * self.number_of_shares\n",
        "\n",
        "                if self.book_value > 0:\n",
        "                    reward = (self.stock_value - self.book_value) / self.book_value * 100\n",
        "                else:\n",
        "                    reward = -1\n",
        "\n",
        "        else:\n",
        "            if action == 0:  # Buy\n",
        "                if self.number_of_shares > 0:\n",
        "                    penalty = -10\n",
        "                # Determining the number of shares the agent can buy.\n",
        "                number_of_shares_to_buy = math.floor(self.investment_capital / self.testing_stock_data[\n",
        "                    'Open'][self.timestep + self.number_of_days_to_consider])\n",
        "                # Adding to the number of shares the agent has.\n",
        "                self.number_of_shares += number_of_shares_to_buy\n",
        "\n",
        "                # Computing the stock value, book value, investment capital and reward.\n",
        "                if number_of_shares_to_buy > 0:\n",
        "                    self.stock_value += \\\n",
        "                        self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
        "                        * number_of_shares_to_buy\n",
        "                    self.book_value += \\\n",
        "                        self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
        "                        * number_of_shares_to_buy\n",
        "                    self.investment_capital -= \\\n",
        "                        self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
        "                        * number_of_shares_to_buy\n",
        "\n",
        "                    reward = 1 + penalty\n",
        "\n",
        "                else:\n",
        "                    # Computing the stock value and reward.\n",
        "                    self.stock_value = self.training_stock_data['Open'][\n",
        "                                           self.timestep + self.number_of_days_to_consider] * self.number_of_shares\n",
        "                    reward = -10\n",
        "\n",
        "            if action == 1:  # Sell\n",
        "                # Computing the investment capital, sell value and reward.\n",
        "                self.investment_capital += \\\n",
        "                    self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
        "                    * self.number_of_shares\n",
        "                sell_value = self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
        "                             * self.number_of_shares\n",
        "\n",
        "                if self.book_value > 0:\n",
        "                    reward = (sell_value - self.book_value) / self.book_value * 100\n",
        "                else:\n",
        "                    reward = -10\n",
        "\n",
        "                self.number_of_shares = 0\n",
        "                self.stock_value = 0\n",
        "                self.book_value = 0\n",
        "\n",
        "            if action == 2:  # Hold\n",
        "                # Computing the stock value and reward.\n",
        "                self.stock_value = self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
        "                                   * self.number_of_shares\n",
        "\n",
        "                if self.book_value > 0:\n",
        "                    reward = (self.stock_value - self.book_value) / self.book_value * 100\n",
        "                else:\n",
        "                    reward = -1\n",
        "\n",
        "        # Determining if the agent currently has shares of the stock or not.\n",
        "        if self.number_of_shares > 0:\n",
        "            stock_held = 1\n",
        "            stock_not_held = 0\n",
        "        else:\n",
        "            stock_held = 0\n",
        "            stock_not_held = 1\n",
        "\n",
        "        # Getting the observation vector.\n",
        "        if self.train:\n",
        "            # If the task is to train the agent the maximum timesteps will be equal to the number of days considered\n",
        "            # subtracted from the  length of the training stock data.\n",
        "            self.max_timesteps = len(self.training_stock_data) - self.number_of_days_to_consider\n",
        "\n",
        "            # Calculating whether the price increased or decreased/remained the same on the majority of days the agent\n",
        "            # considers.\n",
        "            price_increase_list = []\n",
        "            for i in range(self.number_of_days_to_consider):\n",
        "                if self.training_stock_data['Close'][self.timestep + 1 + i] \\\n",
        "                        - self.training_stock_data['Close'][self.timestep + i] > 0:\n",
        "                    price_increase_list.append(1)\n",
        "                else:\n",
        "                    price_increase_list.append(0)\n",
        "\n",
        "            if (np.sum(price_increase_list) / self.number_of_days_to_consider) >= 0.5:\n",
        "                price_increase = 1\n",
        "                price_decrease = 0\n",
        "            else:\n",
        "                price_increase = 0\n",
        "                price_decrease = 1\n",
        "\n",
        "            # Observation vector.\n",
        "            observation = [price_increase, price_decrease, stock_held, stock_not_held]\n",
        "\n",
        "        else:\n",
        "            # If the task is to evaluate the trained agent's performance the maximum timesteps will be equal to the\n",
        "            # number of days considered subtracted from the  length of the testing stock data.\n",
        "            self.max_timesteps = len(self.testing_stock_data) - self.number_of_days_to_consider\n",
        "\n",
        "            # Calculating whether the price increased or decreased/remained the same on the majority of days the agent\n",
        "            # considers.\n",
        "            price_increase_list = []\n",
        "            for i in range(self.number_of_days_to_consider):\n",
        "                if self.testing_stock_data['Close'][self.timestep + 1 + i] \\\n",
        "                        - self.testing_stock_data['Close'][self.timestep + i] > 0:\n",
        "                    price_increase_list.append(1)\n",
        "                else:\n",
        "                    price_increase_list.append(0)\n",
        "\n",
        "            if (np.sum(price_increase_list) / self.number_of_days_to_consider) >= 0.5:\n",
        "                price_increase = 1\n",
        "                price_decrease = 0\n",
        "            else:\n",
        "                price_increase = 0\n",
        "                price_decrease = 1\n",
        "\n",
        "            # Observation vector.\n",
        "            observation = [price_increase, price_decrease, stock_held, stock_not_held]\n",
        "\n",
        "        self.timestep += 1  # Increasing the number of steps taken by the agent by 1.\n",
        "\n",
        "        if np.array_equal(observation, [1, 0, 0, 1]):\n",
        "            observation = 0\n",
        "        if np.array_equal(observation, [1, 0, 1, 0]):\n",
        "            observation = 1\n",
        "        if np.array_equal(observation, [0, 1, 0, 1]):\n",
        "            observation = 2\n",
        "        if np.array_equal(observation, [0, 1, 1, 0]):\n",
        "            observation = 3\n",
        "\n",
        "        # Computing the total account value.\n",
        "        self.total_account_value = self.investment_capital + self.stock_value\n",
        "        # Appending the total account value of the list to plot the graph.\n",
        "        self.total_account_value_list.append(self.total_account_value)\n",
        "\n",
        "        # The episode terminates when the number of infected people becomes greater than 75 % of the population.\n",
        "        done = True if (self.timestep >= self.max_timesteps) \\\n",
        "            else False\n",
        "\n",
        "        info = {}\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        \"\"\"This method renders the agent's total account value over time.\n",
        "\n",
        "        :param mode: 'human' renders to the current display or terminal and returns nothing.\"\"\"\n",
        "\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        plt.plot(self.total_account_value_list, color='lightseagreen', linewidth=3)\n",
        "        plt.xlabel('Days', fontsize=18)\n",
        "        plt.ylabel('Total Account Value', fontsize=18)\n",
        "        plt.title('Total Account Value over Time', fontsize=22)\n",
        "        plt.grid()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4751115",
      "metadata": {
        "id": "d4751115"
      },
      "source": [
        "### TO DO: Implement the Q-learning algorithm "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "ca06c6ab",
      "metadata": {
        "id": "ca06c6ab"
      },
      "outputs": [],
      "source": [
        "# Implementing Q-Learning for the stock trading environment.\n",
        "class QLearning:\n",
        "    #This class implements the Q-learning algorithm\n",
        "\n",
        "    def __init__(self, environment):\n",
        "        #This method instantiates the Q-learning parameters.\n",
        "        #param environment: - This is the environment which needs to be solved.\n",
        "\n",
        "        #Hyper-parameter declaration and initialization \n",
        "        self.environment = environment\n",
        "        self.noofepisodes = 1100\n",
        "        self.alpha = 0.11\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 0.99\n",
        "        self.epsilon_init = 1\n",
        "        self.epsilon_end = 0.1\n",
        "        self.decay = 0.005\n",
        "        self.rewards_list = []\n",
        "        self.epsilon_list = [] \n",
        "        self.q_table = np.zeros((self.environment.observation_space.n,self.environment.action_space.n))\n",
        "\n",
        "    def train(self):\n",
        "        #This method implements the Q-learning algorithm and performs the agent training.\n",
        "\n",
        "        for i in range(self.noofepisodes):\n",
        "          current_state = self.environment.reset()\n",
        "          current_reward = 0\n",
        "          local_done = False\n",
        "          while local_done is False:\n",
        "            self.num = np.random.uniform(0,1)\n",
        "\n",
        "            #Deciding between greedy and random policies to decide between exploration and exploitation\n",
        "            if(self.num < self.epsilon): \n",
        "              action = self.environment.action_space.sample()\n",
        "            else:\n",
        "              action = np.argmax(self.q_table[current_state,:])\n",
        "\n",
        "            next_state, reward_local, local_done, info = self.environment.step(action)\n",
        "\n",
        "            #Formula to update Q-table\n",
        "            self.q_table[current_state,action] = (1 - self.alpha) * self.q_table[current_state,action] + \\\n",
        "            self.alpha * (reward_local + self.gamma * (max(self.q_table[next_state,:])))\n",
        "\n",
        "            current_reward += reward_local\n",
        "            current_state = next_state\n",
        "          self.rewards_list.append(current_reward)\n",
        "          self.epsilon_list.append(self.epsilon)\n",
        "\n",
        "          #Deciding next value of epsilon using decay_rate\n",
        "          self.epsilon = self.epsilon_end + (self.epsilon_init - self.epsilon_end) * np.exp(-self.decay * i)\n",
        "\n",
        "    def evaluate(self):\n",
        "        #Method to evaluate the trained agent's performance by selecting only the greedy/best action in each state\n",
        "\n",
        "        self.environment.train = False\n",
        "        local_done = False\n",
        "        current_state = self.environment.reset()\n",
        "        while local_done is False:\n",
        "            action = np.argmax(self.q_table[current_state,:])\n",
        "            next_state, reward_local, local_done, info = self.environment.step(action)\n",
        "            current_state = next_state\n",
        "        self.environment.render()\n",
        "\n",
        "    def plot(self):\n",
        "\n",
        "        #Method to plot the reward dynamics and epsilon decay\n",
        "\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        plt.plot(self.epsilon_list, color='red', linewidth=1)\n",
        "        plt.xlabel('Episode', fontsize=18)\n",
        "        plt.ylabel('Epsilon', fontsize=18)\n",
        "        plt.title('Epsilon over Episodes', fontsize=22)\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        plt.plot(self.rewards_list,'.', color='blue', linewidth=1)\n",
        "        plt.xlabel('Episode', fontsize=18)\n",
        "        plt.ylabel('Rewards', fontsize=18)\n",
        "        plt.title('Rewards over Episodes', fontsize=22)\n",
        "        plt.grid()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f0269eb",
      "metadata": {
        "id": "2f0269eb"
      },
      "outputs": [],
      "source": [
        "stock_trading_environment = StockTradingEnvironment('./NVDA.csv', number_of_days_to_consider=10)\n",
        "qlearning = QLearning(stock_trading_environment)\n",
        "qlearning.train()\n",
        "qlearning.evaluate()\n",
        "qlearning.plot()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}